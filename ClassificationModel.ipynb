{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TMobile.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FdOzm2ct6SHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "94f57c03-8c08-468b-ba8e-77289d2526da"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "!pip install pytorch-pretrained-bert\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.123)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.123 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.123)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.123->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WBLHwj8gZB5i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "BERT = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def getLatentVector(input):\n",
        "\n",
        "  BERT.eval()\n",
        "\n",
        "  # Tokenized input\n",
        "  tokenized_text = tokenizer.tokenize(input)\n",
        "\n",
        "  # Convert token to vocabulary indices\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "  # print(indexed_tokens)\n",
        "  segments_ids = [0]*(len(indexed_tokens))\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Predict hidden states features for each layer\n",
        "  encoded_layers, _ = BERT(tokens_tensor, segments_tensors)\n",
        "  # We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
        "  assert len(encoded_layers) == 12\n",
        "  return _"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wFP4Yqex8mTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TwoLayerNet(nn.Module):\n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    \"\"\"\n",
        "    In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "    member variables.\n",
        "    \n",
        "    D_in: input dimension\n",
        "    H: dimension of hidden layer\n",
        "    D_out: output dimension\n",
        "    \"\"\"\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    self.linear1 = nn.Linear(D_in, H) \n",
        "    self.linear2 = nn.Linear(H, D_out)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    In the forward function we accept a Variable of input data and we must \n",
        "    return a Variable of output data. We can use Modules defined in the \n",
        "    constructor as well as arbitrary operators on Variables.\n",
        "    \"\"\"\n",
        "    h_relu = F.elu(self.linear1(x))\n",
        "    y_pred = F.softmax(self.linear2(h_relu))\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y7qShP6lGXS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = pd.read_csv('tmobile_data.csv')\n",
        "dataset = np.array(dataset)\n",
        "\n",
        "X = dataset[0:25, 0:1]\n",
        "y = dataset[0:24, 1]\n",
        "\n",
        "X_encoded = []\n",
        "for i in range(len(X)):\n",
        "  X_encoded.append(getLatentVector(str(X[i][0])))\n",
        "\n",
        "y_encoded = []\n",
        "for i in range(len(y)): \n",
        "  tensor = torch.LongTensor([y[i]])\n",
        "  y_encoded.append(tensor)\n",
        "  \n",
        "#split input and output\n",
        "X_train = X_encoded[0:24]\n",
        "y_train = y_encoded\n",
        "X_test = X_encoded[24:]\n",
        "y_test = dataset[24:, 1] \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LzmGIIPd_5p1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2811
        },
        "outputId": "505994c6-2480-4d0c-83a4-0e15305d825e"
      },
      "cell_type": "code",
      "source": [
        "N, D_in, H, D_out = 2, 768, 500, 5\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  \n",
        "\n",
        "iter = 0\n",
        "for epoch in range(N):\n",
        "  for i in range(len(X_train)):\n",
        "    \n",
        "    inputs = X_train[i]\n",
        "    labels = y_train[i]\n",
        "        \n",
        "    # Forward pass to get output/logits\n",
        "    outputs = model(inputs)\n",
        "    \n",
        "    print(outputs)\n",
        "    print(labels)\n",
        "    \n",
        "    # Calculate Loss: softmax --> cross entropy loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Getting gradients w.r.t. parameters\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    # Updating parameters\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    \n",
        "    print('Iteration: {}. Loss: {}'.format(iter, loss.item()))\n",
        "  \n",
        "  print(\"Next Epoch\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1646, 0.2200, 0.1974, 0.2295, 0.1883]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([4])\n",
            "Iteration: 0. Loss: 1.6213830709457397\n",
            "tensor([[0.1548, 0.2193, 0.2434, 0.1989, 0.1837]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([3])\n",
            "Iteration: 0. Loss: 1.6110379695892334\n",
            "tensor([[0.1436, 0.2008, 0.2029, 0.2304, 0.2223]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.6069830656051636\n",
            "tensor([[0.1320, 0.1782, 0.2133, 0.2532, 0.2233]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.6782821416854858\n",
            "tensor([[0.0956, 0.1525, 0.1996, 0.2775, 0.2747]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.6122926473617554\n",
            "tensor([[0.1187, 0.1276, 0.2431, 0.2782, 0.2324]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.5683858394622803\n",
            "tensor([[0.0914, 0.1101, 0.2389, 0.2720, 0.2876]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.7214200496673584\n",
            "tensor([[0.1343, 0.1096, 0.3115, 0.2463, 0.1982]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.6778168678283691\n",
            "tensor([[0.2191, 0.1278, 0.2967, 0.2492, 0.1072]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.515345811843872\n",
            "tensor([[0.0899, 0.0604, 0.3755, 0.2148, 0.2593]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.7261855602264404\n",
            "tensor([[0.1812, 0.0797, 0.4214, 0.1994, 0.1182]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.7370387315750122\n",
            "tensor([[0.1074, 0.0384, 0.5664, 0.1653, 0.1225]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.7211278676986694\n",
            "tensor([[0.1535, 0.0577, 0.5700, 0.1565, 0.0623]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.2589653730392456\n",
            "tensor([[0.1394, 0.0435, 0.5932, 0.1224, 0.1015]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([3])\n",
            "Iteration: 0. Loss: 1.7086842060089111\n",
            "tensor([[0.1035, 0.0208, 0.6796, 0.0844, 0.1117]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8212050199508667\n",
            "tensor([[0.1868, 0.0581, 0.5453, 0.1152, 0.0946]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.6395766735076904\n",
            "tensor([[0.1112, 0.0133, 0.8029, 0.0471, 0.0255]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8488091230392456\n",
            "tensor([[0.1459, 0.0174, 0.7241, 0.0596, 0.0530]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8315550088882446\n",
            "tensor([[0.1202, 0.0091, 0.8022, 0.0414, 0.0271]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.7419013977050781\n",
            "tensor([[0.1083, 0.0049, 0.8514, 0.0248, 0.0106]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.763221025466919\n",
            "tensor([[0.1568, 0.0057, 0.7857, 0.0269, 0.0249]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([4])\n",
            "Iteration: 0. Loss: 1.8348273038864136\n",
            "tensor([[0.1617, 0.0065, 0.7965, 0.0219, 0.0134]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8553261756896973\n",
            "tensor([[0.3888, 0.0088, 0.5043, 0.0437, 0.0545]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.4422805309295654\n",
            "tensor([[0.2196, 0.0070, 0.7391, 0.0189, 0.0154]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8463183641433716\n",
            "Next Epoch\n",
            "tensor([[0.3514, 0.0194, 0.3681, 0.0453, 0.2158]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([4])\n",
            "Iteration: 0. Loss: 1.6043307781219482\n",
            "tensor([[0.3220, 0.0024, 0.6591, 0.0127, 0.0037]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([3])\n",
            "Iteration: 0. Loss: 1.83286452293396\n",
            "tensor([[0.3376, 0.0013, 0.6526, 0.0060, 0.0025]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.1929702758789062\n",
            "tensor([[0.4066, 0.0017, 0.5814, 0.0078, 0.0024]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.4343127012252808\n",
            "tensor([[0.5980, 0.0011, 0.3929, 0.0055, 0.0026]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.4490456581115723\n",
            "tensor([[0.5467, 0.0009, 0.4476, 0.0035, 0.0013]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.3926336765289307\n",
            "tensor([[0.8162, 0.0012, 0.1722, 0.0064, 0.0040]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.049663782119751\n",
            "tensor([[0.6522, 0.0026, 0.3356, 0.0064, 0.0033]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.1932085752487183\n",
            "tensor([[0.0387, 0.0060, 0.9510, 0.0027, 0.0016]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 0.941836416721344\n",
            "tensor([[9.3993e-01, 4.0563e-04, 5.5571e-02, 2.1215e-03, 1.9765e-03]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 0.9503881335258484\n",
            "tensor([[0.2760, 0.0034, 0.7152, 0.0033, 0.0021]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.848374605178833\n",
            "tensor([[9.2070e-01, 2.0278e-04, 7.7619e-02, 1.0346e-03, 4.4568e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 0.9653770327568054\n",
            "tensor([[4.1308e-01, 1.2665e-03, 5.8359e-01, 1.6031e-03, 4.5632e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([2])\n",
            "Iteration: 0. Loss: 1.2581095695495605\n",
            "tensor([[9.0536e-01, 7.2375e-04, 9.0909e-02, 1.9615e-03, 1.0431e-03]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([3])\n",
            "Iteration: 0. Loss: 1.8808155059814453\n",
            "tensor([[9.7673e-01, 1.0287e-04, 2.2194e-02, 4.2816e-04, 5.4389e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8989070653915405\n",
            "tensor([[0.5644, 0.0054, 0.4202, 0.0047, 0.0052]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 1.275528907775879\n",
            "tensor([[9.0221e-01, 1.5301e-04, 9.7161e-02, 3.3905e-04, 1.3399e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.882012963294983\n",
            "tensor([[9.8673e-01, 1.2426e-04, 1.2365e-02, 4.3916e-04, 3.3810e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.9013564586639404\n",
            "tensor([[9.9008e-01, 4.3806e-05, 9.5523e-03, 2.0352e-04, 1.1596e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 0.9122353792190552\n",
            "tensor([[9.8598e-01, 2.4691e-05, 1.3851e-02, 1.0724e-04, 3.6610e-05]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 0.9153139591217041\n",
            "tensor([[9.9802e-01, 1.4651e-05, 1.8120e-03, 7.2361e-05, 7.8506e-05]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([4])\n",
            "Iteration: 0. Loss: 1.9042490720748901\n",
            "tensor([[9.8381e-01, 6.5413e-05, 1.5857e-02, 1.6081e-04, 1.0557e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.900689721107483\n",
            "tensor([[9.9972e-01, 9.3485e-06, 7.8227e-05, 7.9416e-05, 1.1302e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([0])\n",
            "Iteration: 0. Loss: 0.9050406813621521\n",
            "tensor([[9.7430e-01, 1.3349e-04, 2.5068e-02, 2.2292e-04, 2.7991e-04]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "tensor([1])\n",
            "Iteration: 0. Loss: 1.8982839584350586\n",
            "Next Epoch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b0-O2_HZvtft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "13e02d8b-5d25-482b-ec5e-f2b3d1082a04"
      },
      "cell_type": "code",
      "source": [
        "#Testing our toy example: \"What kind of phone plans do you have for small businesses?\"\n",
        "print(model(X_test[0]))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[9.9218e-01, 1.7513e-05, 7.7347e-03, 4.6146e-05, 2.3258e-05]],\n",
            "       grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}